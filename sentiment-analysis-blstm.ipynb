{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\n# Plot libraries\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:45.863436Z","iopub.execute_input":"2024-08-16T09:47:45.864334Z","iopub.status.idle":"2024-08-16T09:47:45.871599Z","shell.execute_reply.started":"2024-08-16T09:47:45.864291Z","shell.execute_reply":"2024-08-16T09:47:45.870189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:46.619610Z","iopub.execute_input":"2024-08-16T09:47:46.620686Z","iopub.status.idle":"2024-08-16T09:47:46.626612Z","shell.execute_reply.started":"2024-08-16T09:47:46.620641Z","shell.execute_reply":"2024-08-16T09:47:46.625245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = '../input/sentiment140/training.1600000.processed.noemoticon.csv'","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:46.914406Z","iopub.execute_input":"2024-08-16T09:47:46.914844Z","iopub.status.idle":"2024-08-16T09:47:46.919937Z","shell.execute_reply.started":"2024-08-16T09:47:46.914812Z","shell.execute_reply":"2024-08-16T09:47:46.918767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv(file_path, encoding=DATASET_ENCODING , names=DATASET_COLUMNS)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:47.118750Z","iopub.execute_input":"2024-08-16T09:47:47.119176Z","iopub.status.idle":"2024-08-16T09:47:52.052140Z","shell.execute_reply.started":"2024-08-16T09:47:47.119126Z","shell.execute_reply":"2024-08-16T09:47:52.050970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.054205Z","iopub.execute_input":"2024-08-16T09:47:52.054580Z","iopub.status.idle":"2024-08-16T09:47:52.068256Z","shell.execute_reply.started":"2024-08-16T09:47:52.054551Z","shell.execute_reply":"2024-08-16T09:47:52.067254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset[['sentiment' , 'text']]","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.069622Z","iopub.execute_input":"2024-08-16T09:47:52.070011Z","iopub.status.idle":"2024-08-16T09:47:52.109496Z","shell.execute_reply.started":"2024-08-16T09:47:52.069968Z","shell.execute_reply":"2024-08-16T09:47:52.108367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.111980Z","iopub.execute_input":"2024-08-16T09:47:52.112369Z","iopub.status.idle":"2024-08-16T09:47:52.123768Z","shell.execute_reply.started":"2024-08-16T09:47:52.112338Z","shell.execute_reply":"2024-08-16T09:47:52.122609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax = ax.set_xticklabels(['Negative','Positive'], rotation=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.124987Z","iopub.execute_input":"2024-08-16T09:47:52.125392Z","iopub.status.idle":"2024-08-16T09:47:52.670659Z","shell.execute_reply.started":"2024-08-16T09:47:52.125362Z","shell.execute_reply":"2024-08-16T09:47:52.669356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining regex patterns.\nurlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\nuserPattern       = '@[^\\s]+'\nhashtagPattern    = '#[^\\s]+'\nalphaPattern      = \"[^a-z0-9<>]\"\nsequencePattern   = r\"(.)\\1\\1+\"\nseqReplacePattern = r\"\\1\\1\"\n\n\ndef preprocess_apply(tweet):\n\n    tweet = tweet.lower()\n\n    # Replace all URls with '<url>'\n    tweet = re.sub(urlPattern,'<url>',tweet)\n    # Replace @USERNAME to '<user>'.\n    tweet = re.sub(userPattern,'<user>', tweet)\n    \n    # Replace 3 or more consecutive letters by 2 letter.\n    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n    # Adding space on either side of '/' to seperate words (After replacing URLS).\n    tweet = re.sub(r'/', ' / ', tweet)\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.672667Z","iopub.execute_input":"2024-08-16T09:47:52.673419Z","iopub.status.idle":"2024-08-16T09:47:52.682342Z","shell.execute_reply.started":"2024-08-16T09:47:52.673376Z","shell.execute_reply":"2024-08-16T09:47:52.681042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['processed_text'] = dataset.text.apply(preprocess_apply)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:47:52.683885Z","iopub.execute_input":"2024-08-16T09:47:52.684335Z","iopub.status.idle":"2024-08-16T09:48:40.669575Z","shell.execute_reply.started":"2024-08-16T09:47:52.684299Z","shell.execute_reply":"2024-08-16T09:48:40.668628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor row in dataset.itertuples():\n    print(\"Text:\", row[2])\n    print(\"Processed:\", row[3])\n    count+=1\n    if count>10:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:40.670892Z","iopub.execute_input":"2024-08-16T09:48:40.671211Z","iopub.status.idle":"2024-08-16T09:48:40.678688Z","shell.execute_reply.started":"2024-08-16T09:48:40.671185Z","shell.execute_reply":"2024-08-16T09:48:40.677595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processedtext = list(dataset['processed_text'])\ndata_pos = processedtext[800000:]\ndata_neg = processedtext[:800000]","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:40.680240Z","iopub.execute_input":"2024-08-16T09:48:40.680662Z","iopub.status.idle":"2024-08-16T09:48:40.960829Z","shell.execute_reply.started":"2024-08-16T09:48:40.680626Z","shell.execute_reply":"2024-08-16T09:48:40.959701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:40.964843Z","iopub.execute_input":"2024-08-16T09:48:40.965171Z","iopub.status.idle":"2024-08-16T09:48:40.969776Z","shell.execute_reply.started":"2024-08-16T09:48:40.965144Z","shell.execute_reply":"2024-08-16T09:48:40.968712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])\n\ny_data = np.where(y_data == 4, 1, 0)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n                                                    test_size = 0.05, random_state = 0)\nprint('Data Split done.')","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:40.970981Z","iopub.execute_input":"2024-08-16T09:48:40.971349Z","iopub.status.idle":"2024-08-16T09:48:41.361797Z","shell.execute_reply.started":"2024-08-16T09:48:40.971315Z","shell.execute_reply":"2024-08-16T09:48:41.360672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nEmbedding_dimensions = 100\n\n# Creating Word2Vec training dataset.\nWord2vec_train_data = list(map(lambda x: x.split(), X_train))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:41.363396Z","iopub.execute_input":"2024-08-16T09:48:41.363816Z","iopub.status.idle":"2024-08-16T09:48:50.894432Z","shell.execute_reply.started":"2024-08-16T09:48:41.363782Z","shell.execute_reply":"2024-08-16T09:48:50.893374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the model and training it.\nword2vec_model = Word2Vec(\n    sentences=Word2vec_train_data,  # The tokenized training data\n    vector_size=Embedding_dimensions,  # Size of the embedding vectors\n    window=5,  # Maximum distance between the current and predicted word within a sentence\n    min_count=5,  # Ignores all words with total frequency lower than this\n    workers=8,  # Number of CPU cores to use for training\n    sg=0  # Use Skip-Gram model (sg=1), or CBOW model (sg=0)\n)\n\nprint(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:48:50.895744Z","iopub.execute_input":"2024-08-16T09:48:50.896059Z","iopub.status.idle":"2024-08-16T09:50:41.345393Z","shell.execute_reply.started":"2024-08-16T09:48:50.896032Z","shell.execute_reply":"2024-08-16T09:50:41.344132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_length = 60\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:50:41.347008Z","iopub.execute_input":"2024-08-16T09:50:41.347394Z","iopub.status.idle":"2024-08-16T09:50:52.561152Z","shell.execute_reply.started":"2024-08-16T09:50:41.347363Z","shell.execute_reply":"2024-08-16T09:50:52.560231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['word_count'] = dataset['processed_text'].apply(lambda x: len(str(x).split()))\n\n# Step 2: Use seaborn to create a boxplot\nplt.figure(figsize=(8, 6))\nsns.boxplot(x=dataset['word_count'])\nplt.title('Boxplot of Word Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:50:52.562538Z","iopub.execute_input":"2024-08-16T09:50:52.563387Z","iopub.status.idle":"2024-08-16T09:50:56.038560Z","shell.execute_reply.started":"2024-08-16T09:50:52.563347Z","shell.execute_reply":"2024-08-16T09:50:56.037511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_length = 90000\n\ntokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\ntokenizer.fit_on_texts(X_data)\ntokenizer.num_words = vocab_length\nprint(\"Tokenizer vocab length:\", vocab_length)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:50:56.039898Z","iopub.execute_input":"2024-08-16T09:50:56.040207Z","iopub.status.idle":"2024-08-16T09:51:27.288324Z","shell.execute_reply.started":"2024-08-16T09:50:56.040181Z","shell.execute_reply":"2024-08-16T09:51:27.287080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\nX_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n\nprint(\"X_train.shape:\", X_train.shape)\nprint(\"X_test.shape :\", X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:51:27.289537Z","iopub.execute_input":"2024-08-16T09:51:27.289840Z","iopub.status.idle":"2024-08-16T09:52:03.201561Z","shell.execute_reply.started":"2024-08-16T09:51:27.289813Z","shell.execute_reply":"2024-08-16T09:52:03.200494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_word_index = {word: token for word, token in tokenizer.word_index.items() if token <= vocab_length}","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:03.203131Z","iopub.execute_input":"2024-08-16T09:52:03.203532Z","iopub.status.idle":"2024-08-16T09:52:03.318397Z","shell.execute_reply.started":"2024-08-16T09:52:03.203494Z","shell.execute_reply":"2024-08-16T09:52:03.317474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_length , Embedding_dimensions))\n\nfor word, token in filtered_word_index.items():\n    if word in word2vec_model.wv:\n        embedding_matrix[token] = word2vec_model.wv[word]\n\nprint(\"Embedding Matrix Shape:\", embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:03.319616Z","iopub.execute_input":"2024-08-16T09:52:03.319948Z","iopub.status.idle":"2024-08-16T09:52:03.796031Z","shell.execute_reply.started":"2024-08-16T09:52:03.319919Z","shell.execute_reply":"2024-08-16T09:52:03.794922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:03.797537Z","iopub.execute_input":"2024-08-16T09:52:03.797859Z","iopub.status.idle":"2024-08-16T09:52:03.805256Z","shell.execute_reply.started":"2024-08-16T09:52:03.797831Z","shell.execute_reply":"2024-08-16T09:52:03.803854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getModel():\n    embedding_layer = Embedding(input_dim = vocab_length,\n                                output_dim = Embedding_dimensions,\n                                weights=[embedding_matrix],\n                                input_length=input_length,\n                                trainable=False)\n\n    model = Sequential([\n        embedding_layer,\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n        Conv1D(100, 5, activation='relu'),\n        GlobalMaxPool1D(),\n        Dense(16, activation='relu'),\n        Dense(1, activation='sigmoid'),\n    ],\n    name=\"Sentiment_Model\")\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:03.807105Z","iopub.execute_input":"2024-08-16T09:52:03.808256Z","iopub.status.idle":"2024-08-16T09:52:03.815771Z","shell.execute_reply.started":"2024-08-16T09:52:03.808212Z","shell.execute_reply":"2024-08-16T09:52:03.814684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_model = getModel()\ntraining_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:03.817067Z","iopub.execute_input":"2024-08-16T09:52:03.819420Z","iopub.status.idle":"2024-08-16T09:52:04.807337Z","shell.execute_reply.started":"2024-08-16T09:52:03.819389Z","shell.execute_reply":"2024-08-16T09:52:04.806348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:04.808776Z","iopub.execute_input":"2024-08-16T09:52:04.809095Z","iopub.status.idle":"2024-08-16T09:52:04.815819Z","shell.execute_reply.started":"2024-08-16T09:52:04.809068Z","shell.execute_reply":"2024-08-16T09:52:04.814674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:04.817228Z","iopub.execute_input":"2024-08-16T09:52:04.817652Z","iopub.status.idle":"2024-08-16T09:52:04.841676Z","shell.execute_reply.started":"2024-08-16T09:52:04.817623Z","shell.execute_reply":"2024-08-16T09:52:04.840785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = training_model.fit(\n    X_train, y_train,\n    batch_size=1024,\n    epochs=12,\n    validation_split=0.1,\n    callbacks=callbacks,\n    verbose=1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T09:52:04.842977Z","iopub.execute_input":"2024-08-16T09:52:04.843373Z","iopub.status.idle":"2024-08-16T10:25:48.335677Z","shell.execute_reply.started":"2024-08-16T09:52:04.843338Z","shell.execute_reply":"2024-08-16T10:25:48.334466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\nloss, val_loss = history.history['loss'], history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:25:48.883777Z","iopub.execute_input":"2024-08-16T10:25:48.884214Z","iopub.status.idle":"2024-08-16T10:25:49.525022Z","shell.execute_reply.started":"2024-08-16T10:25:48.884162Z","shell.execute_reply":"2024-08-16T10:25:49.524012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\ndef ConfusionMatrix(y_pred, y_test):\n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:29:45.751883Z","iopub.execute_input":"2024-08-16T10:29:45.752779Z","iopub.status.idle":"2024-08-16T10:29:45.763493Z","shell.execute_reply.started":"2024-08-16T10:29:45.752743Z","shell.execute_reply":"2024-08-16T10:29:45.762333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting on the Test dataset.\ny_pred = training_model.predict(X_test)\n\n# Converting prediction to reflect the sentiment predicted.\ny_pred = np.where(y_pred>=0.5, 1, 0)\n\n# Printing out the Evaluation metrics. \nConfusionMatrix(y_pred, y_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:31:18.700027Z","iopub.execute_input":"2024-08-16T10:31:18.700648Z","iopub.status.idle":"2024-08-16T10:31:38.920816Z","shell.execute_reply.started":"2024-08-16T10:31:18.700611Z","shell.execute_reply":"2024-08-16T10:31:38.919512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the evaluation metrics for the dataset.\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:32:20.603636Z","iopub.execute_input":"2024-08-16T10:32:20.604060Z","iopub.status.idle":"2024-08-16T10:32:20.740155Z","shell.execute_reply.started":"2024-08-16T10:32:20.604027Z","shell.execute_reply":"2024-08-16T10:32:20.738995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving Word2Vec-Model\nword2vec_model.wv.save('Word2Vec-twitter-100')\nword2vec_model.wv.save_word2vec_format('Word2Vec-twitter-100-trainable')\n\n# Saving the tokenizer\nwith open('Tokenizer.pickle', 'wb') as file:\n    pickle.dump(tokenizer, file)\n\n# Saving the TF-Model.\ntraining_model.save('Sentiment-BiLSTM.h5')\n\n# Saving the model weights with the required extension\ntraining_model.save_weights('weights.weights.h5') ","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:38:30.216111Z","iopub.execute_input":"2024-08-16T10:38:30.216615Z","iopub.status.idle":"2024-08-16T10:38:39.083100Z","shell.execute_reply.started":"2024-08-16T10:38:30.216578Z","shell.execute_reply":"2024-08-16T10:38:39.082209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport pickle\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:52:52.988625Z","iopub.execute_input":"2024-08-16T10:52:52.989055Z","iopub.status.idle":"2024-08-16T10:52:52.994622Z","shell.execute_reply.started":"2024-08-16T10:52:52.989019Z","shell.execute_reply":"2024-08-16T10:52:52.993284Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/Tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\n# Step 2: Load the model\nmodel = load_model('/kaggle/working/Sentiment-BiLSTM.h5')\n\n# Step 3: Prepare your input text\ntext = [\"Today is not a great day!\"]\nsequences = tokenizer.texts_to_sequences(text)\npadded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=60)  # Adjust maxlen to match your training\n\n# Step 4: Predict the sentiment\nprediction = model.predict(padded_sequences)\n\n# Step 5: Interpret the prediction\nsentiment = np.argmax(prediction, axis=1)\nif sentiment == 0:\n    print(\"Negative Sentiment\")\nelif sentiment == 1:\n    print(\"Positive sentiment\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T10:58:48.829016Z","iopub.execute_input":"2024-08-16T10:58:48.830077Z","iopub.status.idle":"2024-08-16T10:58:50.774452Z","shell.execute_reply.started":"2024-08-16T10:58:48.830039Z","shell.execute_reply":"2024-08-16T10:58:50.773346Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step\nNegative Sentiment\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}